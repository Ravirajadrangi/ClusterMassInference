{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm trying to find the most efficient way to vectorie my importance sampling calculation. Since my other notebook is actually running, I'll mess with it here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "colors = sns.color_palette()\n",
    "import numpy as np\n",
    "np.random.seed(0)#\"Random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First, will need the parameters\n",
    "redshift = .9 #one redshift, for now. \n",
    "nSamples = 1000\n",
    "vals = {}\n",
    "z = [0.23, 1.5]\n",
    "vals['Mp'] = [2.0e14, 1.0e14]\n",
    "vals['A'] = [1.944, 0.293]\n",
    "vals['B1'] = [1.97, 3.07]\n",
    "vals['B2'] = [0.7, 1.2]\n",
    "vals['B3'] = [0.40, 0.73]\n",
    "\n",
    "#interpolates naively between the points given in the original paper\n",
    "z_params = {}\n",
    "from scipy.stats import linregress\n",
    "for key, val in vals.iteritems():\n",
    "    slope, intercept, r, p, stderr = linregress(z, val)\n",
    "    z_params[key] = (slope, intercept)\n",
    "\n",
    "#return the values of the parameters at a given z. \n",
    "getMassParams = lambda z : {key:val[0]*z+val[1] for key,val in z_params.iteritems()}\n",
    "\n",
    "def n_approx(m,z):\n",
    "    params = getMassParams(z)\n",
    "    return params['A']*np.exp(-params['B1']*(m/params['Mp']) \\\n",
    "                    -0.5*params['B2']*(m/params['Mp'])**2 \\\n",
    "                    -0.166*params['B3']*(m/params['Mp'])**3)\n",
    "\n",
    "def log_n_approx(m,z):\n",
    "    params = getMassParams(z)\n",
    "    return -1*(params['B1']*(m/params['Mp']) \\\n",
    "                    +0.5*params['B2']*(m/params['Mp'])**2 \\\n",
    "                    +0.166*params['B3']*(m/params['Mp'])**3)+np.log(params['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use MCMC to sample masses from the mass function\n",
    "import emcee as mc\n",
    "ndim = 1\n",
    "nWalkers = 1000\n",
    "nSteps = 500\n",
    "nBurn = 400\n",
    "nCores = 1\n",
    "\n",
    "Mmin, Mmax = 1e13, 5e15\n",
    "pos0 = np.random.uniform(np.log10(Mmin), np.log10(Mmax), size = nWalkers).reshape((nWalkers, ndim))\n",
    "\n",
    "def log_p(logM, z):\n",
    "    logM = logM[0]\n",
    "    m = 10**logM\n",
    "    if m>Mmax or m<Mmin:\n",
    "        return -np.inf\n",
    "    \n",
    "    return log_n_approx(m, z)\n",
    "\n",
    "sampler = mc.EnsembleSampler(nWalkers, ndim, log_p, args=[redshift],threads = nCores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler.run_mcmc(pos0, nSteps);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logMSamples = sampler.chain[:,nBurn:, :].reshape((-1, nSamples))[0,:]\n",
    "mSamples = 10**logMSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_piv = 2.35e14\n",
    "logL0_true = 3.141\n",
    "a_true, b_true = 0.842, -0.03\n",
    "B_l_true = 0.642\n",
    "sigma_l_true = 0.184\n",
    "\n",
    "_A_lam = lambda a,b,z : a*pow((1+z)/1.3, b)\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mv_norm\n",
    "#forward model\n",
    "def logLam(logLam0, a, b, B_lam, z, M):\n",
    "    A_lam = _A_lam(a,b,z)\n",
    "    return logLam0+A_lam*np.log(M/M_piv)+B_lam*np.log((1+z)/1.3)\n",
    "\n",
    "def logLamSample(logLam0, a, b, B_lam,sigma_lam, z,M):\n",
    "    return norm.rvs(loc = logLam(logLam0, a, b, B_lam, z, M), scale = sigma_lam, size = M.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logRichness = logLamSample(logL0_true, a_true, b_true, B_l_true, sigma_l_true, redshift, mSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import logsumexp\n",
    "from scipy.stats import norm, multivariate_normal as mv_norm\n",
    "#returns a logmass given a richness\n",
    "def invLogLam(logLam0, a, b, B_lam, z, logRich):\n",
    "    A_lam = _A_lam(a,b,z)\n",
    "    return np.log(M_piv)+(logRich-logLam0-B_lam*np.log((1+z)/1.3))/A_lam\n",
    "#TODO Make this one synonymous with it non-inverse version\n",
    "#TODO Consider checking if logRich is a vector or a number, and acting accodingly. \n",
    "def invLogLamSample(logLam0, a, b, B_lam,sigma_mass, z,logRich, size = 100):\n",
    "    mu = invLogLam(logLam0, a, b, B_lam, z, logRich)\n",
    "    return mv_norm.rvs(mean = mu, cov = sigma_mass*np.identity(logRich.shape[0]), size =  size).T#(logRich.shape[0], size)\n",
    "    #return norm.rvs(loc = mu, scale = sigma_mass, size =  size)#( size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_liklihood(logL0, a,b, B_l, sigma, z, logRich):\n",
    "    sigma_mass = 1 #TODO idk what this should be; i suppose it's my discretion, or some relation to\n",
    "    logL = 0\n",
    "    #TODO Vectorize furhter?\n",
    "    '''\n",
    "    for lr in logRich:\n",
    "        #use truths as really really good guess. Can relax later. \n",
    "        massSamples = invLogLamSample(logL0_true, a_true, b_true, B_l_true,sigma_mass, z, lr)\n",
    "\n",
    "        logPRich = norm.logpdf(lr, loc =logLam(logL0, a, b, B_l, z, massSamples), scale = sigma)\n",
    "        logPMass = log_n_approx(massSamples,z)\n",
    "        logPMass_sample = norm.logpdf(massSamples, loc = invLogLam(logL0_true, a_true, b_true, B_l_true, z, lr), scale = sigma_mass)\n",
    "        \n",
    "        logL+= logsumexp(logPRich+logPMass-logPMass_sample)\n",
    "        \n",
    "    return logL\n",
    "    \n",
    "    '''\n",
    "    logRichArr = np.array([logRich[:] for i in xrange(100)]).T\n",
    "    massSamples = invLogLamSample(logL0_true, a_true, b_true, B_l_true,sigma_mass, z, logRich)\n",
    "    logPRich = norm.logpdf(logRichArr, loc =logLam(logL0, a, b, B_l, z, massSamples), scale = sigma)\n",
    "    logPMass = log_n_approx(massSamples,z)\n",
    "    logPMass_sample = norm.logpdf(massSamples, loc = invLogLam(logL0_true, a_true, b_true, B_l_true, z, logRichArr), scale = sigma_mass)\n",
    "    return np.sum(logsumexp(logPRich+logPMass-logPMass_sample, axis = 1), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7576972.71993\n",
      "-7577087.53561\n",
      "-7577313.39824\n",
      "-7576761.25612\n",
      "1 loops, best of 3: 1.35 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "print log_liklihood(logL0_true, a_true, b_true, B_l_true, sigma_l_true, redshift, logRichness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.61 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "10000 loops, best of 3: 95.7 µs per loop\n"
     ]
    }
   ],
   "source": [
    "logRichArr = np.array([logRichness[:] for i in xrange(100)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 1.42 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "massSamples = invLogLamSample(logL0_true, a_true, b_true, B_l_true,1, redshift, logRichness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 1.32 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "mu = invLogLam(logL0_true, a_true, b_true, B_l_true, redshift, logRichness)\n",
    "x= mv_norm.rvs(mean = mu, cov = np.identity(logRichness.shape[0]), size =  100).T#(logRich.shape[0], size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 57.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "y = np.zeros((logRichness.shape[0], 100))\n",
    "for i, lr in enumerate(logRichness):\n",
    "    mu = invLogLam(logL0_true, a_true, b_true, B_l_true, redshift, lr)  \n",
    "    y[i, :] = norm.rvs(loc = mu, scale = 1, size =  100)#( size,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that the mv_norm draw is more expensive than i assume. I was speaking to someone about it, and I guess it's not that weird. After all, it's accounting for all the covariances in the data, and can't recognized I passed in an indentity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invLogLamSample2(logLam0, a, b, B_lam,sigma_mass, z,logRich, size = 100):\n",
    "    mu = invLogLam(logLam0, a, b, B_lam, z, logRich)\n",
    "    #return mv_norm.rvs(mean = mu, cov = sigma_mass*np.identity(logRich.shape[0]), size =  size).T#(logRich.shape[0], size)\n",
    "    return norm.rvs(loc = mu, scale = sigma_mass, size =  size)#( size,)\n",
    "\n",
    "def log_liklihood2(logL0, a,b, B_l, sigma, z, logRich):\n",
    "    sigma_mass = 1 #TODO idk what this should be; i suppose it's my discretion, or some relation to\n",
    "    logL = 0\n",
    "\n",
    "    for lr in logRich:\n",
    "        #use truths as really really good guess. Can relax later. \n",
    "        massSamples = invLogLamSample2(logL0_true, a_true, b_true, B_l_true,sigma_mass, z, lr)\n",
    "\n",
    "        logPRich = norm.logpdf(lr, loc =logLam(logL0, a, b, B_l, z, massSamples), scale = sigma)\n",
    "        logPMass = log_n_approx(massSamples,z)\n",
    "        logPMass_sample = norm.logpdf(massSamples, loc = invLogLam(logL0_true, a_true, b_true, B_l_true, z, lr), scale = sigma_mass)\n",
    "        \n",
    "        logL+= logsumexp(logPRich+logPMass-logPMass_sample)\n",
    "        \n",
    "    return logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7577232.25947\n",
      "-7576888.49532\n",
      "-7577014.07498\n",
      "-7577204.86665\n",
      "1 loops, best of 3: 369 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "print log_liklihood2(logL0_true, a_true, b_true, B_l_true, sigma_l_true, redshift, logRichness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma_mass = 1\n",
    "lr = logRichness[0]\n",
    "z = redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 8.78 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "10000 loops, best of 3: 56.2 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#use truths as really really good guess. Can relax later. \n",
    "massSamples = invLogLamSample2(logL0_true, a_true, b_true, B_l_true,sigma_mass, z, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 6.77 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "10000 loops, best of 3: 115 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "logPRich = norm.logpdf(lr, loc =logLam(logL0_true, a_true, b_true, B_l_true, z, massSamples), scale = sigma_l_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 6.62 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "10000 loops, best of 3: 35.5 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "logPMass = log_n_approx(massSamples,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 60.16 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "10000 loops, best of 3: 103 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "logPMass_sample = norm.logpdf(massSamples, loc = invLogLam(logL0_true, a_true, b_true, B_l_true, z, lr), scale = sigma_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 8.90 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "10000 loops, best of 3: 41 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "logL = logsumexp(logPRich+logPMass-logPMass_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
